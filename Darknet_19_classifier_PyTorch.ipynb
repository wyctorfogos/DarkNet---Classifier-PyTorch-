{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKIhDFuPqlooVfkWytNsF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wyctorfogos/DarkNet---Classifier-PyTorch-/blob/main/Darknet_19_classifier_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tqw8gSRgXkCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a48b821-5aad-480e-85a4-a20af2fb01b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7feab2725a30>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam, SGD, RMSprop\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "torch.manual_seed(17)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = transforms.Compose([\n",
        "#    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=0., std=1.)\n",
        "])"
      ],
      "metadata": {
        "id": "JXLqJM5oS3i6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.fakedata import transforms\n",
        "\n",
        "train= datasets.CIFAR10(root='data', download=True, train=True, transform=transformations)\n",
        "test= datasets.CIFAR10(root='data',download=True, train=False, transform=transformations)\n",
        "\n",
        "#train = datasets.VOCDetection(root= './data', year = '2012', image_set = 'train', download = True, transform = transformations)\n",
        "#test = datasets.VOCDetection(root= './data', year = '2012', image_set = 'val', download = True, transform = transformations)\n",
        "\n",
        "batch=1\n",
        "\n",
        "dataset_train = DataLoader(train, batch_size=batch, shuffle=True)\n",
        "dataset_test = DataLoader(test, batch_size=batch, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G9teUhOYsRK",
        "outputId": "be2d1351-3f50-4930-c47c-a811b3458512"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset"
      ],
      "metadata": {
        "id": "_rktX92kpcTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_iter = iter(dataset_train)\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "def plot_images(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# show images\n",
        "plot_images(torchvision.utils.make_grid(images))\n",
        "\n",
        "\n",
        "del images\n",
        "del labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "uELYkaRuohrk",
        "outputId": "89c731f9-e4bb-4a93-deaa-17468602b007"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYnElEQVR4nO2db4yldXXHP+f+mZmd2eXPuhZXxKKWpCGmopkQG42hGg01JmjSEH1BeEFc00pSE/uC0KTSpC+0qRpfNLZrIWKjIhWNpCGtlJgQ36CLRUBpKxKs0IUFdmF3dmbu39MX924ykOecmXnmzr0Dv+8n2eyd53ef5zn3d5/vPPf+vnPOMXdHCPHapzHrAIQQ00FiF6IQJHYhCkFiF6IQJHYhCkFiF6IQWjvZ2cyuBr4CNIF/cvfPZ88//8JD/oY3vrl6cJoOoNUa2jPUmar0dSUHzM5lyUHj/eq+0bVOFg5azTg8iaP+JRztuf2r8dn/+19eevGFyh1ri93MmsDfAx8EngJ+amZ3u/svo33e8MY384/fur9ybNJ+f6Omol+7Yq+naKcZHzNRu9ugertXb4d87odJHCTXjgXnayVxZPPRtziOvsUflJ1hOGYejHn2wbt6tv7suqvCPXbyMf5K4HF3f8Ldu8AdwDU7OJ4QYhfZidgvBn674eenxtuEEHuQXV+gM7MjZnbMzI69dOr53T6dECJgJ2J/Grhkw89vGm97Ge5+1N2X3X35/AsP7eB0QoidsBOx/xS4zMzeYmZzwMeBuycTlhBi0tRejXf3vpndCPw7I+vtNnf/RbaPAc1G9e8XT5ZArcYa+athVX2vkM9Vdj9Ilq2teoV5MEhW462djGUr3XEczWClu5k5EMlYM3UgMscgmcdwNT57Z6rHMq3syGd393uAe3ZyDCHEdNBf0AlRCBK7EIUgsQtRCBK7EIUgsQtRCDtaja+DWZCFlKZXbT/1I7Mg6tpye6U4Z5aAElMz9sgWAhqNLJmkW7k5cdDSjLJhlryUxGhEMSb7ZPObXIuNSadu1k30C9CdXYhCkNiFKASJXYhCkNiFKASJXYhCmMFqfDSQ7FPnPDXH8oO+etNrspXudK6y1ecg2QVgOFyv3N5Kkl0Gdec32a0ROChZIkxWAsuShJZGeg3HcxU5A2kc0fZsLpIIhBCvISR2IQpBYheiECR2IQpBYheiECR2IQphqtabGTRCb2CKNeimmM+Su0nTs/KycmZ5t5jYMppvJwdtVt9HeoPErstK2iUWYBZ/ZA9mHYN8GMuikdwf8zlOrLewO01Wd2/7147u7EIUgsQuRCFI7EIUgsQuRCFI7EIUgsQuRCHsyHozsyeBM8AA6Lv78mb7xNZbeqbt75HVi5ti8lpWz2yaSXR1rBrI67stLcYZbAyrx148XV2bDvK5Suu7ZS9tGMWfvS+xLPIMwX6yX1yvr9GojrE34ZqHk/DZ/8jd1YtZiD2OPsYLUQg7FbsDPzSzB83syCQCEkLsDjv9GP9ed3/azH4HuNfM/svd79/4hPEvgSMAbzh8yQ5PJ4Soy47u7O7+9Pj/E8D3gSsrnnPU3ZfdffmCg4d2cjohxA6oLXYzWzKzA+ceAx8CHp1UYEKIybKTj/EXAd8f2yUt4Fvu/m/5LhZbb2n7p+1nyuW+1vTS3vZI0ltaKJHEFppr98Kx8/bPh2OdtcCWy9pJpe9ZYl0lr20YZNn1hrEF2JxfiseS/dqNTji2by62KYdBlt2L6/F1mmXYRdQWu7s/Abyj7v5CiOki602IQpDYhSgEiV2IQpDYhSgEiV2IQphuwUlqZnpN3KKanue1d9rDJVleSXbV0r6439jivvjy6a1X30caFh+v0axTlBGCpDEAotO1WknBSYvtxnY7zmw7bym2Iudac+HYyVNrlduz7Lusr1y4z/Z3EUK8GpHYhSgEiV2IQpDYhSgEiV2IQpjqajzUq0GXNf4JR5J2Qeli/IRzZLK6alMlaIME0ErGDiztC8fazXhl3YLl4maydG7JKnjDkgSUdrzS3W5Wj3W6SS28RrLivn8xHJufj+V0ZiVOkun0q52GhsWvK0qEyS433dmFKASJXYhCkNiFKASJXYhCkNiFKASJXYhCmK71ZvEf8Kcl6Oqcqm4Nugk7ZXunBl2cSLKQdHFaXEgukWwao7FhbGtlySn7l+Ig980thGP9bnWSSTO5zy0mba3m2rHd2Avq3QF0uvHrHgaxZJdwHUtXd3YhCkFiF6IQJHYhCkFiF6IQJHYhCkFiF6IQNrXezOw24CPACXd/+3jbQeA7wKXAk8C17n5q02MBFvg1E7eokqy3uo6XJ7Xa6pxsmvlwDYutn/1JLbn5Vnw/8GFsNXnQ5qnbjbO/FhIP8MD+OPvO+/H70ulVv+75uSRTrpG8z8PYwhzEU0w3iANgGJwutd5q6Ggrd/avA1e/YttNwH3ufhlw3/hnIcQeZlOxj/utn3zF5muA28ePbwc+OuG4hBATpu539ovc/fj48TOMOroKIfYwO16g89EX2fBLjpkdMbNjZnbs1Mnndno6IURN6or9WTM7DDD+/0T0RHc/6u7L7r584cHX1zydEGKn1BX73cD148fXAz+YTDhCiN1iK9bbt4GrgENm9hTwOeDzwJ1mdgPwG+DarZ4wtAayApHRXnVtranWgEwswF0oRhmdrd2sl1GWtRnqB4USR4FURzIYxPv0k6yx7Oro9WNbq9OpHmskxTL7gW0I+d0xm4619bjAJa3qrL06xVkzNhW7u38iGPrARCMRQuwq+gs6IQpBYheiECR2IQpBYheiECR2IQph6r3eptX6bM+0WJt2HIFtlBWOnE+KKA4SWyuzvPr96r5tmfW2th73euv24v26STHHyM1rtubDfYaJDZwk+rHeiePv9+Md5+ar53/Sl47u7EIUgsQuRCFI7EIUgsQuRCFI7EIUgsQuRCFM3Xqr0+styqDKyLPe6vWBsxpmiFlsGbnHx/PkrWmQ2Dit6rHzl+ICi9l09BKrqduLd+x0q+dxmLzmTmJdra4mNl833o+geOQwuaY6K3FRzNZcbFOudePMNmvE99VWkIGXxVgH3dmFKASJXYhCkNiFKASJXYhCkNiFKIQpr8Z7WGsuK0HnaX26gJor7vkx6+0WHq5moTxLVuMPLFWv7O5LEmGGSc21rK5aN2m71AuW8bMWWllizcrKejjmw3gVfF+yeh7x0umVcKw1F0/Iejdrh7V9lydq8VQX3dmFKASJXYhCkNiFKASJXYhCkNiFKASJXYhC2Er7p9uAjwAn3P3t4223AJ8EzrVlvdnd79n0WMSGkic2Q2O6/ZomirF962e0X2xDtYNkF4DFxepWQpnN188stMR7S9suBckp/aQGXWY0rZw9G441krlaXDivesDi92Vufl841hskCTSdOIGGxvYTkaxWAlimo835OnB1xfYvu/sV43+bCl0IMVs2Fbu73w+cnEIsQohdZCff2W80s4fN7DYzu3BiEQkhdoW6Yv8q8DbgCuA48MXoiWZ2xMyOmdmxkyefr3k6IcROqSV2d3/W3QfuPgS+BlyZPPeouy+7+/LBg4fqximE2CG1xG5mhzf8+DHg0cmEI4TYLbZivX0buAo4ZGZPAZ8DrjKzKxi5JU8Cn9ryGQM7IbMMQtfoVeHIxb9P86ymuK7avqBdEMSthKI2SAD9XhxHt1uv7VJkyw2GyWtObj2dpL4bw3iuorP1kz5O3ohlkV1yvV5sD84vxHvWqstYIyNuU7G7+ycqNt+67TMJIWaK/oJOiEKQ2IUoBIldiEKQ2IUoBIldiEKYevsnwuKGiZUQpAVZ0kpor9hyWTafE9ta7UZsa+1fnA/HGsFcdfqx1dRJCiV2OtvPbIM4Wy5taZQM9TPvMLHRer3qOFrtJJsvOVdmN3aTLMB9QYsnoJYdXacWpe7sQhSCxC5EIUjsQhSCxC5EIUjsQhSCxC5EIUzdegvthBrF9XL/YW94b26xjWNJZtviQvx7eGE+ftv6gdXU6cRxrK/HGWVrydh6Jx7r9qrHBolNZmnDv3ismVTTXF1bq9yeGHl04reFtfW4qOQgKabZbMTvZ9hrr6ZNGaE7uxCFILELUQgSuxCFILELUQgSuxCFMIPV+GrSxcVoVTLraVRrdX83iNd9m8147MCBpXAsqlkGsBYkrqyvx0kaq6vr8dhaPJYlfkRJMpa0Xaqb95El10Sr8WeT19zzOMbMTfDEAbI61+qEHSrd2YUoBIldiEKQ2IUoBIldiEKQ2IUoBIldiELYSvunS4BvABcxWtc/6u5fMbODwHeASxm1gLrW3U9tdrzQTaiR0+KJNeFpqkNikSR17SL3xDJ7zePkiPOW4lpyC/Nz4Vg3qf22tlY9tnKm2oICOLu6Gp+rl9RcS2q19YKad61WO9wnqyVHklAUJpIAg8BGy2radYIkntG5wiHm2wvhWMOSNmDBQdNrOLPyohi28Jw+8Fl3vxx4N/BpM7scuAm4z90vA+4b/yyE2KNsKnZ3P+7uPxs/PgM8BlwMXAPcPn7a7cBHdytIIcTO2dZ3djO7FHgn8ABwkbsfHw89w+hjvhBij7JlsZvZfuAu4DPufnrjmI++PFd+8TCzI2Z2zMyOnTz5/I6CFULUZ0tiN7M2I6F/092/N978rJkdHo8fBk5U7evuR9192d2XDx48NImYhRA12FTsNvoL/luBx9z9SxuG7gauHz++HvjB5MMTQkyKrWS9vQe4DnjEzB4ab7sZ+Dxwp5ndAPwGuHbTI7njmb0S7ldjl6yeWUpmy0UWSWyvzbXi42VtnKK2RQArZ+M6aGdWqsdWzsZZXp1e0sYpsaiydzLKAMvs0kYy957Ya9kYgeWVXx3JPTA5V1ZnLhkK489ah9VhU7G7+4+JFfCBiUYjhNg19Bd0QhSCxC5EIUjsQhSCxC5EIUjsQhTCVAtOOp62yIlIi/WFO2Vj9doMDQOzqdGI91nYF2evDRLzamUlzlI7sxLbaCtnq7PUOkn22iDNHozHeskxIzspbfGUFmXMhrZ/zLrWW5rhmNlriT0bzb+nGZiBpZi8MN3ZhSgEiV2IQpDYhSgEiV2IQpDYhSgEiV2IQpiu9TZ0Op3qYn6NrIFZHRIbJ7PyLLNdrNo+abWywovhECdPnQnHukkm2srZpODkanUs3X5s/XQHcRZd9q4MkwzGZmCxWWJBWVJU0j2x+YaJrdWvnqthUBBzNJhk5iUFRBsevy/rayvh2Op69X5zc4vhPs1mdSHNPKtQCFEEErsQhSCxC1EIErsQhSCxC1EIU12Nx+JV934/Xm3NVn3DUzWqVys3HUtWW92rnYT19Xg1ey2pFxet7gNYI35rzpyOk2T6/er5TZthJU5IM3U14qO2w/DjFetG8r70BnFLpnYzjnG+XT3WbsZtqFZX4pXz3vrZeMziGNfX4v36Vh1LoxnXKIwMA63GCyEkdiFKQWIXohAkdiEKQWIXohAkdiEKYVPrzcwuAb7BqCWzA0fd/StmdgvwSeC58VNvdvd7smP1+32efz7o5JpZBsEf/WcJLZ78Hmu147pwjaQ6Wb+/Wrm92Yxtw6bHNlkzqV3XaMTWEIkNFbWomp9fiM/VjC8DT2oGttqxVRa5eeuduH6eJ6/L+/E8LuyLE0YW2tXXwWrSDqtlsV16YH/8vpx6IelSPLc/HNr/ujdWbs/qyYXWW7zLlnz2PvBZd/+ZmR0AHjSze8djX3b3v9vCMYQQM2Yrvd6OA8fHj8+Y2WPAxbsdmBBismzrO7uZXQq8E3hgvOlGM3vYzG4zswsnHJsQYoJsWexmth+4C/iMu58Gvgq8DbiC0Z3/i8F+R8zsmJkde+nFUxMIWQhRhy2J3czajIT+TXf/HoC7P+vuAx91A/gacGXVvu5+1N2X3X35/At08xdiVmwqdhsted8KPObuX9qw/fCGp30MeHTy4QkhJsVWVuPfA1wHPGJmD4233Qx8wsyuYLTa/yTwqc0O1O32+O1TT1eOZW2BIosttd48qyWXZL2l5kW1NXTBBfvCPZZix4tGYjUt7IuPmVlvUXbVeXOHwn3m23F21YkXToRjS4vxizuwv9oOWzv7UrjPMyeeCcc8yUacX4jj2L90QeX2rHWVBdmNACzEtm1nNX5tC+34/VxbrbZ01wKrF6AZ2KVZe7WtrMb/mOq6g6mnLoTYW+gv6IQoBIldiEKQ2IUoBIldiEKQ2IUohOm2fwIGQUMhT4pKRoUNE+cNvKa9lliAUZuhUy/FGVlO0sLH49fc7b0QjmXW0CAofnn2dLgLnsTRasXz0ekmGX1BUlkzrkTJ4mI8V2dX4yKQZ87Eltf6erV91QoyKQHmkmy+7lxsbS2cF1uY3SRrb/3kc5Xb13rxvXgYpMT1k7ZhurMLUQgSuxCFILELUQgSuxCFILELUQgSuxCFMFXrzTDMqn+/BJvH+0XWUJIp10iy3pLfcZmdFybSJTudWU2KKPbi/l9zFo/Nt2KrrNcJ+tF1Y8tobTU+V6uVFL5MONurtpoGgzjbrN2IYzxvX5xtNphPiou2qm00DyxggNZcfK7uILa21teSbMRuUiS0dX71gMdxpMmZAbqzC1EIErsQhSCxC1EIErsQhSCxC1EIErsQhTBV6w2DZmCJDQexneSBz5D1wmokfdQ8tPLyDLDIHkwS5dLChg2P7Z/5xQPhWMviY/b61cfsJ3F01+KssfZcbL0tLS6FY9EcW1I4smXxWGZ5NRPrsxXcz4ZJ0dH1TjxX60FxSIBBP7GCW3FGXDPqZUgcY2QdZtax7uxCFILELkQhSOxCFILELkQhSOxCFMKmq/FmtgDcD8yPn/9dd/+cmb0FuAN4HfAgcJ171jfnHMGKZdbKaRgkzyTJDCQr3dHqPoAnS/w+qB5rJFk8jVY8xXOteDU7yz8ZBEkmAMNGkDzRjg846tsZnCt5bWuDeI6bQdZQNve9XuyErHcTlyRp9RWWmkvcmuByGx0vaePUSurr9YfxWC9o2WSJOxEllGX5MVu5s3eA97v7Oxi1Z77azN4NfAH4srv/HnAKuGELxxJCzIhNxe4jzpX2bI//OfB+4Lvj7bcDH92VCIUQE2Gr/dmb4w6uJ4B7gV8DL7r7ub8+eAq4eHdCFEJMgi2J3d0H7n4F8CbgSuD3t3oCMztiZsfM7NjK6RdrhimE2CnbWo139xeBHwF/CFxgZudWHd4EVDZed/ej7r7s7sv7z6vulS2E2H02FbuZvd7MLhg/3gd8EHiMkej/ZPy064Ef7FaQQoids5VEmMPA7WbWZPTL4U53/1cz+yVwh5n9DfCfwK2bHsmdYZRokllvgcXWaGQtnjKyVlNZfbpqYyNKZABoJtbb/Hwy/UHLq9FB46H2QvUxo7ZbAP1+fK5BYAsB9JKWXdEUD4eJ5TWMk0W8GY9lNQW9WW0rZglPPogd5FYj8UST5JphMv+DwO5tJElDYV3GxDreVOzu/jDwzortTzD6/i6EeBWgv6ATohAkdiEKQWIXohAkdiEKQWIXohAsy/Ka+MnMngN+M/7xEPD81E4eozhejuJ4Oa+2OH7X3V9fNTBVsb/sxGbH3H15JidXHIqjwDj0MV6IQpDYhSiEWYr96AzPvRHF8XIUx8t5zcQxs+/sQojpoo/xQhTCTMRuZleb2X+b2eNmdtMsYhjH8aSZPWJmD5nZsSme9zYzO2Fmj27YdtDM7jWzX43/v3BGcdxiZk+P5+QhM/vwFOK4xMx+ZGa/NLNfmNmfj7dPdU6SOKY6J2a2YGY/MbOfj+P46/H2t5jZA2PdfMfMguqiAe4+1X+MEjR/DbwVmAN+Dlw+7TjGsTwJHJrBed8HvAt4dMO2vwVuGj++CfjCjOK4BfiLKc/HYeBd48cHgP8BLp/2nCRxTHVOGGVo7x8/bgMPAO8G7gQ+Pt7+D8Cfbue4s7izXwk87u5P+Kj09B3ANTOIY2a4+/3AyVdsvoZR4U6YUgHPII6p4+7H3f1n48dnGBVHuZgpz0kSx1TxERMv8joLsV8M/HbDz7MsVunAD83sQTM7MqMYznGRux8fP34GuGiGsdxoZg+PP+bv+teJjZjZpYzqJzzADOfkFXHAlOdkN4q8lr5A9153fxfwx8Cnzex9sw4IRr/Zyev97yZfBd7GqEfAceCL0zqxme0H7gI+4+6nN45Nc04q4pj6nPgOirxGzELsTwOXbPg5LFa527j70+P/TwDfZ7aVd541s8MA4/9PzCIId392fKENga8xpTmxUVuau4Bvuvv3xpunPidVccxqTsbn3naR14hZiP2nwGXjlcU54OPA3dMOwsyWzOzAucfAh4BH8712lbsZFe6EGRbwPCeuMR9jCnNiZsaohuFj7v6lDUNTnZMojmnPya4VeZ3WCuMrVhs/zGil89fAX84ohrcycgJ+DvximnEA32b0cbDH6LvXDYx65t0H/Ar4D+DgjOL4Z+AR4GFGYjs8hTjey+gj+sPAQ+N/H572nCRxTHVOgD9gVMT1YUa/WP5qwzX7E+Bx4F+A+e0cV39BJ0QhlL5AJ0QxSOxCFILELkQhSOxCFILELkQhSOxCFILELkQhSOxCFML/A1RKroZ3BSBbAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Darknet-19 model\n",
        "\n",
        "class BlockConv1(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels, num_kernel=1, size_stride=1, *kwgars):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels,output_channels, kernel_size=num_kernel*3, stride=size_stride, padding=1)\n",
        "    self.batch1 = nn.BatchNorm2d(output_channels)\n",
        "    self.conv2 = nn.Conv2d(output_channels,input_channels, kernel_size=num_kernel, stride=(size_stride), padding=0)\n",
        "    self.batch2 = nn.BatchNorm2d(input_channels)\n",
        "    self.conv3 = nn.Conv2d(input_channels,output_channels, kernel_size=num_kernel*3, stride=size_stride, padding=1)\n",
        "    self.batch3 = nn.BatchNorm2d(output_channels)\n",
        "    self.max_pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.relu= nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    X = self.relu(self.batch1(self.conv1(x)))\n",
        "    X= self.relu(self.batch2(self.conv2(X)))\n",
        "    X = self.relu(self.conv3(X))\n",
        "    X = self.conv3(X)\n",
        "    X = self.relu(self.batch3(self.conv3(x)))\n",
        "    X = self.max_pool(X)\n",
        "    return X\n",
        "\n",
        "class DarkNetClassifier(nn.Module):\n",
        "  def __init__(self, Block1Conv1, input_channels=3, num__kernels=1,  num_stride=1, num_classes=20):\n",
        "    self.num_classes=num_classes\n",
        "\n",
        "    ##Darknet part\n",
        "    super(DarkNetClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels,32, kernel_size = 3*num__kernels, stride = num_stride*3, padding=1, bias=False)\n",
        "    self.batch1= nn.BatchNorm2d(32)\n",
        "    self.max_pool1=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    self.conv2 = nn.Conv2d(32,64, kernel_size = (3*num__kernels), stride = num_stride*3, padding=1, bias=False)\n",
        "    self.batch2= nn.BatchNorm2d(64)\n",
        "    self.max_pool2=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    ##self.block1 = BlockConv1(input_channels=64,output_channels=128,num_kernel=(num__kernels), size_stride=num_stride)\n",
        "    ##self.block2 = BlockConv1(input_channels=128,output_channels=256,num_kernel=(num__kernels), size_stride=num_stride)\n",
        "    self.conv1_block1 = nn.Conv2d(64, 128, kernel_size=num__kernels*3, stride=num_stride, padding=1)\n",
        "    self.batch1_block1 = nn.BatchNorm2d(128)\n",
        "    self.conv2_block1 = nn.Conv2d(128,64, kernel_size=num__kernels, stride=(num_stride), padding=0)\n",
        "    self.batch2_block1 = nn.BatchNorm2d(64)\n",
        "    self.conv3_block1 = nn.Conv2d(64,128, kernel_size=num__kernels*3, stride=num_stride, padding=1)\n",
        "    self.batch3_block1 = nn.BatchNorm2d(128)\n",
        "    self.max_pool1_block1=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv1_block2 = nn.Conv2d(128, 256, kernel_size=num__kernels*3, stride=num_stride, padding=1)\n",
        "    self.batch1_block2 = nn.BatchNorm2d(256)\n",
        "    self.conv2_block2 = nn.Conv2d(256,128, kernel_size=num__kernels, stride=(num_stride), padding=0)\n",
        "    self.batch2_block2 = nn.BatchNorm2d(128)\n",
        "    self.conv3_block2 = nn.Conv2d(128,256, kernel_size=num__kernels*3, stride=num_stride, padding=1)\n",
        "    self.batch3_block2 = nn.BatchNorm2d(256)\n",
        "    self.max_pool1_block2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "  \n",
        "    \n",
        "    self.conv3 = nn.Conv2d(256,512, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch3= nn.BatchNorm2d(512)\n",
        "    self.conv4 = nn.Conv2d(512,256, kernel_size = (num__kernels), stride = (num_stride), padding=0, bias=False)\n",
        "    self.batch4= nn.BatchNorm2d(256)\n",
        "    self.conv5 = nn.Conv2d(256,512, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch5= nn.BatchNorm2d(512)\n",
        "    self.conv6 = nn.Conv2d(512,256, kernel_size = (num__kernels), stride = (num_stride), padding=1, bias=False)\n",
        "    self.batch6= nn.BatchNorm2d(256)\n",
        "    self.conv7 = nn.Conv2d(256,512, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch7= nn.BatchNorm2d(512)\n",
        "    self.max_pool3=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    self.conv8 = nn.Conv2d(512, 1024, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch8= nn.BatchNorm2d(1024)\n",
        "    self.conv9 = nn.Conv2d(1024, 512, kernel_size = (num__kernels), stride = num_stride, padding=0, bias=False)\n",
        "    self.batch9= nn.BatchNorm2d(512)\n",
        "    self.conv10 = nn.Conv2d(512, 1024, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch10= nn.BatchNorm2d(1024)\n",
        "    self.conv11 = nn.Conv2d(1024, 512, kernel_size = (num__kernels), stride = (num_stride), padding=1, bias=False)\n",
        "    self.batch11= nn.BatchNorm2d(512)\n",
        "    self.conv12 = nn.Conv2d(512, 1024, kernel_size = (3*num__kernels), stride = num_stride, padding=1, bias=False)\n",
        "    self.batch12 = nn.BatchNorm2d(1024)\n",
        "\n",
        "    self.conv_last_layer=nn.Conv2d(1024,num_classes,kernel_size=(1,1), stride = (1,1), padding=1, bias=False)\n",
        "\n",
        "    self.flatten= nn.Flatten()\n",
        "    self.soft=nn.Softmax(dim=1)\n",
        "\n",
        "    self.relu= nn.ReLU(0.1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #model= self.soft(self.fc(self.avg(self.conv_last_layer(self.conv12(self.conv11(self.conv10(self.conv9(self.conv8(self.max_pool3(self.conv7(self.conv6(self.conv5(self.conv4(self.conv3(self.block2(self.block1(self.max_pool2(self.conv2(self.max_pool1(self.conv1(x)))))))))))))))))))))\n",
        "    X = self.relu(self.batch1(self.conv1(x)))\n",
        "    X = self.max_pool1(X)\n",
        "    X = self.relu(self.batch2(self.conv2(X)))\n",
        "    X = self.max_pool2(X)\n",
        "\n",
        "    X = self.relu(self.batch1_block1(self.conv1_block1(X)))\n",
        "    X = self.relu(self.batch2_block1(self.conv2_block1(X)))\n",
        "    X = self.relu(self.batch3_block1(self.conv3_block1(X)))\n",
        "    x = self.max_pool1_block1(X)\n",
        "    \n",
        "    X = self.relu(self.batch1_block2(self.conv1_block2(X)))\n",
        "    X = self.relu(self.batch2_block2(self.conv2_block2(X)))\n",
        "    X = self.relu(self.batch3_block2(self.conv3_block2(X)))\n",
        "    X = self.max_pool1_block2(X)\n",
        "\n",
        "\n",
        "    ##X = self.block1(X)\n",
        "    ##X = self.block2(X)\n",
        "\n",
        "    X = self.relu(self.batch7(self.conv7(self.relu(self.batch6(self.conv6(self.relu(self.batch5(self.conv5(self.relu(self.batch4(self.conv4(self.relu(self.batch3(self.conv3(X)))))))))))))))\n",
        "    X = self.max_pool3(X)\n",
        "    X = self.conv_last_layer(self.relu(self.batch12(self.conv12(self.relu(self.batch11(self.conv11(self.relu(self.batch10(self.conv10(self.relu(self.batch9(self.conv9(self.relu(self.batch8(self.conv8(X))))))))))))))))\n",
        "    \n",
        "    ##Classifier part\n",
        "    N = X.data.size(0)\n",
        "    C = X.data.size(1)\n",
        "    H = X.data.size(2)\n",
        "    W = X.data.size(3)\n",
        "\n",
        "    #print(N,C,H,W)\n",
        "\n",
        "    x = nn.AvgPool2d(X, (H, W))\n",
        "    X = X.view(-1, (N*C*H*W))\n",
        "    X = nn.Linear((N*C*H*W), self.num_classes)(X)\n",
        "    X = self.soft(X)\n",
        "    #X = nn.Linear((N*C*H*W), self.num_classes)(X)\n",
        "\n",
        "    \n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "id": "sO_oNXPQZcBm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test():\n",
        "  model=DarkNetClassifier(BlockConv1, input_channels=3, num__kernels=1,  num_stride=1, num_classes=10)\n",
        "  X=torch.rand(1, 3, 224, 224)\n",
        "#  X.permute(0, 3, 1, 2)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "  Y = model(X).to(device)\n",
        "#  Y = model.to(device)\n",
        "  print(Y)\n",
        "  #model.to(device)\n",
        "  return model"
      ],
      "metadata": {
        "id": "kAo5Hw5nJ2lf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=test()\n",
        "del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOeNwT1FMgFv",
        "outputId": "0ee95479-fdf1-4a9d-f860-7be74ea7188e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0972, 0.0772, 0.0989, 0.1034, 0.1314, 0.0916, 0.0871, 0.1147, 0.1035,\n",
            "         0.0949]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.max_pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    ##activation function\n",
        "    self.relu= nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    X = self.relu(self.conv1(x))\n",
        "    X= self.max_pool(X)\n",
        "    X = self.relu(self.conv2(X))\n",
        "    X= self.max_pool(X)\n",
        "    X = X.view(-1 ,16*5*5)\n",
        "    X= self.fc1(X)\n",
        "    X = self.fc2(X)\n",
        "    model =self.fc3(X)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def test_ConvClassifier():\n",
        "  model=ConvClassifier(num_classes=10)\n",
        "  X=torch.rand(1,3,32,32)\n",
        "  #X.permute(0, 3, 1, 2)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "  Y = model(X).to(device)\n",
        "  print(torch.argmax(Y))\n",
        "  #model.to(device)\n",
        "  return model"
      ],
      "metadata": {
        "id": "M1-e8EGjc_CZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=test_ConvClassifier()\n",
        "\n",
        "del model"
      ],
      "metadata": {
        "id": "qh7qBU0Tq-vA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e4a767-aca2-4b7b-c738-17e4eda44c7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names={'person':0, 'bird': 1, 'cat':2, 'cow':3, 'dog':4, 'horse':5, 'sheep':6, 'aeroplane':7, 'bicycle':8, 'boat':9, 'bus':10, 'car':11, 'motorbike':12, 'train':13, 'bottle':14, 'chair':15, 'dining table':16, 'potted plant':17, 'sofa':18, 'tv/monitor':19}"
      ],
      "metadata": {
        "id": "4v0fYpM4MesM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number=class_names.get('bicycle')\n",
        "print(number)"
      ],
      "metadata": {
        "id": "EWhWo2y6NzUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191b18cb-6da1-4eea-c3d6-282e626f7ffe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_images(imgs, x_min, y_min, x_max, y_max):\n",
        "  image=transforms.ToPILImage()(imgs)\n",
        "  plt.subplot(2,2,1)\n",
        "  plt.imshow(image, interpolation=\"bicubic\")\n",
        "  plt.subplot(2,2,2)\n",
        "  \n",
        "  cropped_image=transforms.ToPILImage()(imgs)\n",
        "  width = abs(int(x_min)-int(x_max))\n",
        "  height = abs(int(y_min)-int(y_max))\n",
        "\n",
        "  # Select area to crop\n",
        "  area = (int(x_min), int(y_min), int(x_max), int(y_max))\n",
        "    \n",
        "  cropped_image=cropped_image.crop(area)\n",
        "  plt.imshow(cropped_image)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SMTWeUbrWoBe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_info(annotations):\n",
        "  obj_name=annotations[\"annotation\"]['object'][-1]['name']\n",
        "  obj_coord=annotations[\"annotation\"]['object'][-1]['bndbox']\n",
        "  x_min, y_min, x_max, y_max = obj_coord['xmin'], obj_coord['ymin'], obj_coord['xmax'], obj_coord['ymax']\n",
        "  return x_min, y_min, x_max, y_max, obj_name, obj_coord "
      ],
      "metadata": {
        "id": "DVAJZl6EANbJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "epochs=25\n",
        "\n",
        "model= model=DarkNetClassifier(BlockConv1, input_channels=3, num__kernels=1,  num_stride=1, num_classes=10)\n",
        "#test_ConvClassifier()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_QdpcmvtlxV",
        "outputId": "4496595a-8ca5-406f-b17b-44f79d940d59"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DarkNetClassifier(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1), bias=False)\n",
              "  (batch1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1), bias=False)\n",
              "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv1_block1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (batch1_block1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_block1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (batch2_block1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_block1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (batch3_block1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (max_pool1_block1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv1_block2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (batch1_block2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2_block2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (batch2_block2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3_block2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (batch3_block2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (max_pool1_block2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (batch4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv8): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv9): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (batch9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv10): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv11): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv12): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_last_layer): Conv2d(1024, 10, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (soft): Softmax(dim=1)\n",
              "  (relu): ReLU(inplace=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "loss_functionn= torch.nn.CrossEntropyLoss()\n",
        "opt=Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "k=0\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ##for imgs, annotations in dataset_train.dataset:\n",
        "###  for i, (imgs, annotations) in enumerate(dataset_train):\n",
        "  for batch in dataset_train:\n",
        "    imgs, annotations = batch\n",
        "    #imgs = list(img.to(device) for img in imgs)\n",
        "    #annotations = dict(annotations.items())\n",
        "\n",
        "    ##x_min, y_min, x_max, y_max, obj_name, obj_coord = select_info(annotations)\n",
        "    ##print(obj_name)\n",
        "    ##print(obj_coord)\n",
        "    ##print(x_min, y_min, x_max, y_max)\n",
        "    \n",
        "\n",
        "    ##Y=class_names.get(obj_name)\n",
        "    \n",
        "    ##Crop images\n",
        "    ##crop_images(imgs, x_min, y_min, x_max, y_max)\n",
        "\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    annotations = annotations.to(device)\n",
        "\n",
        "    imgs=transforms.Resize((224,224))(imgs)\n",
        "    imgs = imgs.to(device)\n",
        "    \n",
        "    #X=transforms.ToPILImage()(X)\n",
        "    #plt.imshow(X)\n",
        "    #plt.show()\n",
        "\n",
        "    #X=X.squeeze(axis=0)\n",
        "    ##X=X.permute(2, 1, 0)\n",
        "    \n",
        "    Y_pred= model(imgs).to(device)\n",
        "    #Y_pred=torch.reshape(Y_pred, (-1,1))\n",
        "\n",
        "    ##print(torch.argmax(Y_pred))\n",
        "    ##print(annotations)\n",
        "\n",
        "    loss= loss_functionn(Y_pred,annotations)\n",
        "      \n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    #del annotations\n",
        "    #del Y_pred\n",
        "    #del imgs\n",
        "  print(\"Epoch:{}, loss:{}\".format(epoch, loss.item()))"
      ],
      "metadata": {
        "id": "Ou2sDIgEdNcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_layers=0\n",
        "conv_layers=[]\n",
        " \n",
        "model_children=list(model.children())\n",
        " \n",
        "for child in model_children:\n",
        "  if type(child)==nn.Conv2d:\n",
        "    no_of_layers+=1\n",
        "    conv_layers.append(child)\n",
        "  elif type(child)==nn.Sequential:\n",
        "    for layer in child.children():\n",
        "      if type(layer)==nn.Conv2d:\n",
        "        no_of_layers+=1\n",
        "        conv_layers.append(layer)\n",
        "print(no_of_layers)"
      ],
      "metadata": {
        "id": "WUXNdodvRhCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = [conv_layers[0](imgs)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "outputs = results"
      ],
      "metadata": {
        "id": "TFo8LrQZU41W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "for num_layer in range(len(outputs)):\n",
        "    plt.figure(figsize=(50, 10))\n",
        "    layer_viz = outputs[num_layer][0, :, :, :]\n",
        "    layer_viz = layer_viz.data\n",
        "    print(\"Layer \",num_layer+1)\n",
        "    for i, filter in enumerate(layer_viz):\n",
        "        if i == 16: \n",
        "            break\n",
        "        plt.subplot(2, 8, i + 1)\n",
        "        image = cv2.cvtColor(abs(filter.cpu().numpy()), cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "4J-8JPqZWo8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_viz[0]"
      ],
      "metadata": {
        "id": "NVnnPohXaKow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}